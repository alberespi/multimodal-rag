{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 24,
  "text": "8 CHAPTER 2. RELATED WORK\nrecent method along these lines is the Energy Density-Induced Clustering (EDIC) al-\ngorithm [26]. EDIC extracts features via singular value decomposition of local image\npatches and finds that ship targets have distinct single energy concentrations compared\nto clutter. By applying K-means clustering on these features (with no ground-truth\nlabels), EDIC can separate ships from sea with high accuracy. Impressively, it was\nreported to match or surpass state-of-the-art deep learning detectors in de-\ntection rate and processing efficiency on various SAR scenes [26]. This suggests that\ncarefully designed unsupervised techniques can still compete, especially when labeled\ndata is scarce or when real-time performance on limited hardware is required. Similarly,\ngenerative models and autoencoders have been explored to detect vessels as anomalies:\nfor instance, training a deep autoencoder on “empty ocean” images such that when\na ship is present, reconstruction error is high, flagging the location. While not yet\nas prevalent as supervised CNN methods, these unsupervised and hybrid approaches\nhighlight the ongoing innovation to reduce false alarms and dependence on labels.\n2.1.4 Foundation Models and Transfer Learning\nThe concept of foundation models overlaps with the practice of transfer learning that\nis already common in computer vision, Almost all modern vessel detection networks\nleverage pre-trained backbones - for example, a ResNet or EfficientNet trained on Im-\nageNet is sued as the feature extractor and then fine-tuned on a ship dataset. This\ntransfer learning has been shown to significantly improve performance and conver-\ngence speed [21, 22]. In Li et al.’s work, they dine-tuined a Faster R-CNN on the SSDD\nSAR ship dateset with an ImageNet-pretrained CNN and other optimizations, achiev-\ning an ∼8% mAP boost over training from scratch [21, 22]. Such results exemplify the\nvalue of using a rich foundation of learned features.\nRecently, larger and more comprehensive foundation models have emerged in the\nremote sensing domain [8]. These include models explicitly trained on satellite imagery\nin an unsupervised manner to learn general representations. For example, theSeasonal\nContrast (SeCo) model (2021) used contrastive self-supervised learning on one mil-\nlion Sentinel-2 images to create a robust ResNet backbone [27]. Likewise, SimCLR,\nMoCo, DINO and MAE frameworks have been applied to build generic feature\nextractors for overhead imagery [8, 28, 29, 30, 31]. When fine-tuned for tasks like ship\ndetection, such pre-trained models often outperform those trained on natural images\nbecause they have already seen a variety of Earth observation conditions (e.g., different\nwater textures, cloud appearances, sensor noise) during pre-training. In one survey,\nresearchers found that self-supervised pre-training (contrastive learning, masked au-\ntoencoders, etc.) significantly enhances object detection performance in remote sensing\nby improving model robustness [8, 28, 32, 30, 31]. In practice, this means fewer target-\nspecific labels are needed to reach high accuracy, and the model is more generalizable\nto new locations or imaging conditions. Foundation models can also be multi-modal:\nfor instance, vision-language models akin to CLIP have been trained on satellite im-\nages paired with geo-text (like ship reports or context descriptions) in an unsupervised\nway [33, 34]. Such models could enable zero-shot detection or classification of vessels\nby leveraging textual prompts (e.g., detecting “fishing boats” vs “cargo ships” without\nexplicit training on those classes). Another example is Meta AI’s Segment Anything\nModel (SAM) [35], a foundation model for segmentation which, if applied to maritime\nimages, could allow segmenting ships with minimal user input. While these multi-",
  "image": "page_024.png"
}