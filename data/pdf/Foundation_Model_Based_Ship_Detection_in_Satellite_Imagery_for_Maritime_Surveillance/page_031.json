{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 31,
  "text": "2.3. EVALUATION FRAMEWORKS 15\n• False Alarm Rate: Especially for surveillance applications, false alarm or false\npositive rate is monitored. This could be given as number of false detections per\nimage or per square kilometer. A low false alarm rate is crucial in practice to\nensure trust in the system outputs. Some papers report the probability of detec-\ntion (Pd) at a certain false alarm rate (Pfa), which comes from radar community\nmetrics.\n• Computational Metrics: To evaluate efficiency, metrics likeinference time per\nimage, frames per second (FPS), or GFLOPs are cited. For example, a method\nmight be said to run at 10 FPS on 1024x1024 images on an NVIDIA V100\nGPU. Model size (number of parameters or memory footprint) is also noted when\nrelevant, as it affects deployability. The UOW-Vessel paper, for instance, provides\nmodel size and inference speed for tested algorithms as part of the benchmark to\ncompare not just accuracy but also resource usage [56].\n• Robustness Metrics: Some evaluations include how performance changes with\nperturbations (like adding noise, varying resolution) to assess robustness. While\nnot a single metric, it’s an evaluation strategy to test generalization.\nGround truth annotation quality heavily influences these metrics. One challenge\nin vessel datasets is annotation consistency - e.g., how to mark a ship that is only\npartly visible at the image edge, or two ships very close together. Inconsistent or\ncoarse annotations (like a single box covering two adjacent boxes) can penalize an\nalgorithm unfairly or make the metric less informative. The SSDD initial version\nhad some coarse labels that were later refined in the official release for more precise\nevaluation [21]. Similarly, differences in annotation formats (bounding box vs polygon)\nmean that algorithms should ideally be evaluated in the same format they output.\n2.3.3 Challenges in Annotation & Validation\nLabeling satellite images for vessel detection is labor-intensive. Large-scale dataset of-\nten use a combination of manual annotation and semi-automated tools. For instance,\nthe Airbus challenge [53] provided segmentation masks that were likely produced by\nmanual tracing and quality control on thousands of images - a huge effort. In some\ncases, annotation errors exist (e.g., a ship missed by annotators or false label for some-\nthing that is not a ship). This can impact validation: an algorithm might detect a\nreal ship that the ground truth missed and be penalizes as a “false positive” when it\nwas actually correct. To mitigate this, some evaluation protocols allow a tolerance for\nunlabeled objects or employ multiple human reviews of test images. Cross-validation\n(splitting data into train/val folds) is used when dataset size is limited, to ensure results\nare not a fluke of a particular split.\nAnother validation strategy is cross-dataset evaluation: train in one dataset and\ntest on another to gauge generalization. For example, training on MASATI and testing\non Airbus, or vice versa. f a model still performs well, it indicates robustness. This\nkind of evaluation is increasingly encouraged given the desire for models that work on\nglobal scale data.\nIn summary, rigorous evaluation of vessel detection involves using standardized\ndatasets with clear train/test splits and reporting metrics like precision, recall, F1 and\nAP/mAP, along with computational efficiency. Proper validation should consider the",
  "image": "page_031.png"
}