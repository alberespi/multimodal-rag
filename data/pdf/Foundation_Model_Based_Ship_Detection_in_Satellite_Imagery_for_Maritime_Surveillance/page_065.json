{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 65,
  "text": "Chapter 6\nConclusions & Future Work\nThe final chapter distills the technical results of Chapters 3-5 into a concise set of\nconclusions and outlines promising directions for extension. It first revisits the objec-\ntives stated in Chapter 2, evaluating how far they have been met, and then discusses\nstrategic opportunities for improving accuracy, robustness, and operational integration.\n6.1 Conclusions\nThis work set out to explore whether a foundation-model paradigm—self-supervised\npre-training followed by task-specific fine-tuning—could reduce the annotation burden\ntraditionally associated with ship detection in medium-resolution optical imagery. The\nevidence collected throughout the thesis supports four main conclusions.\n1. Foundation models do pay for themselves.Pre-training a masked-autoencoder\non ∼60 000 unlabeled PlanetScope patches yields an encoder whose features allow\nthe downstream ship-segmentation task to reach InstanceIoU 0.536 and Instance\nF1 0.671 with only 280 labeled patches. Compared with a baseline trained from\nscratch, this represents a gain of +0.155 IoU and +0.309 F1 for roughly one-tenth\nof the manual labeling effort.\n2. Overlap-aware objectives are critical in extreme class-imbalance sce-\nnarios. The hybrid BCE + IoU loss outperforms focal, Dice, and plain BCE\nalternatives, halving false positives while keeping recall above 0.64. Directly\noptimizing the instance overlap metric proves more effective than minimizing\naverage pixel error when vessels occupy <0.1% of the image area.\n3. Qualitative robustness extends beyond numerical metrics.Multi-spectral\ninputs (six PlanetScope bands) help the network disambiguate wakes, thin clouds,\nand pier structures—failure modes prevalent in RGB-only baselines. Although\nthe model tends to over-segment hull boundaries by 10–35%, its predictions re-\nmain visually coherent and operationally useful for alerting.\n4. Near-real-time deployment is feasible. Batch inference processes ≈16 six-\nband patches per 50 ms on a single NVIDIA A40, implying scene-level runtimes\ncompatible with the MARVISION ingestion rate. The 61 M-parameter network\n(≈245 MB ONNX) fits comfortably within the memory budget of the target\nprocessing node.\n49",
  "image": "page_065.png"
}