{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 17,
  "text": "Chapter 1\nIntroduction\n1.1 Motivation\nMaritime surveillance is a critical application of Earth observation, with satellite im-\nagery offering wide-area monitoring of vessel traffic for purposes such as fisheries en-\nforcement, pollution control and border security [2]. Detecting ships from space, how-\never, poses significant challenges. Targets tend to be very small relative to the image\nscale and can appear as only a few pixels, especially in medium-resolution imagery (e.g.,\nPlanetScope or Sentinel-2). The ocean background is complex and dynamic - waves,\nsun glint and weather effects create clutter that makes reliable detection difficult [2].\nIndeed, sea-surface clutter can obscure or mimic the signature of small vessels, leading\nto false alarms or missed detections under high sea states [2]. These factors, combined\nwith the need to survey vast areas, make automated ship detection in satellite imagery\nan inherently challenging task.\nConventional deep learning approaches to this problem require large annotated\ndatasets to achieve high accuracy. Labeling satellite images for small objects like ships\nis labor-intensive and costly, as it demands expert security of high-resolution data and\noften auxiliary information (e.g. AIS signals) to confirm vessel presence. As a re-\nsult, obtaining sufficient labeled training data is a major bottleneck [3]. Furthermore,\nsupervised models trained on one region or sensor may not generalize well to new condi-\ntions without extensive retraining, due to variations in imaging sensors, environments\nand acquisition conditions. There is a pressing need for methods that can reduce the\ndependence on labeled data while improving robustness to varying scenarios.\nFoundation models have emerged as a promising solution to these issues. A\nfoundation model is broadly defined as “ any model trained on broad data (typically\nusing self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range\nof downstream tasks ” [4]. In essence, these are large-scale models (often based on\nCNN or Transformer architectures) pretrained on vast unlabeled datasets, learning\nrich and generalizable feature representations. They can then be fine-tuned with\nrelatively few labeled examples for specific tasks, leveraging the learned general features\nto achieve strong performance even in data-sparse regimes [3, 5]. Such models have\ndemonstrated remarkable generalization and few-shot learning capabilities in computer\nvision and natural language domains [3], and recent studies indicate they can set new\nbenchmarks in remote sensing tasks like scene classification, semantic segmentation\nand object detection [5]. For instance, a model pretrained on hundreds of thousands\nof unlabeled satellite images can learn invariant features for recognizing ships under\n1",
  "image": "page_017.png"
}