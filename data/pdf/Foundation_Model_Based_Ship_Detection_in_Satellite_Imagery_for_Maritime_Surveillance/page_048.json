{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 48,
  "text": "32 CHAPTER 4. EXPERIMENTAL SETUP AND RESULTS\nwhich loss function allows the foundation model to learn more meaningful and gener-\nalizable representations.\nFigure 4.1: Training and validation loss curves during self-supervised pretraining using\nBCELoss. Although the loss stabilizes without overfitting, it converges to higher values\nand exhibits mild fluctuations after epoch 100.\nFigure 4.1plots the training and validation reconstruction loss of the self-supervised\nmodel when optimized with Binary Cross-Entropy (BCE) over 500 epochs (linear\nscale). Both curves exhibit a steep drop during the first ≈ 20 epochs—falling from\n≈ 2.8 × 10−1 to ≈ 2.25 × 10−1 —followed by a gradual, almost asymptotic decrease.\nAfter epoch 100 the curves flatten and approach a stable plateau: the training loss set-\ntles around 0.223 and the validation loss around 0.220. The small gap between both\ncurves suggests no signs of overfitting, indicating that the model retains good general-\nization. However, the relatively high loss values imply that BCE is not fully exploiting\nthe potential of the reconstruction task, likely due to its binary-oriented formulation\nnot aligning well with the continuous nature of multispectral image intensities.\nFigure 4.2: Log-scale training and validation loss curves using MSELoss. The model\nconverges quickly and smoothly to values in the range of 10 −4, demonstrating signifi-\ncantly lower reconstruction error and stable generalization..\nIn contrast, Figure 4.2 displays the training and validation loss curves for the\nmodel trained with MSELoss (on a logarithmic scale). The loss drops quickly dur-\ning the first ≈ 50 epochs, decreasing from over 10 −3 to below 2 × 10−4, and then",
  "image": "page_048.png"
}