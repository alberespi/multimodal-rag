{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 40,
  "text": "24 CHAPTER 3. MATERIALS & METHODS\nstructural design of convolutional, residual, and attention blocks as the pretraining\ndecoder) and produces a binary segmentation mask as output. In essence, the fine-\ntuning model is architecturally identical to the pretraining model, except that its final\noutput layer is a 1 ×1 convolution with sigmoid activation yielding a probability map\n(where each pixel indicates the confidence of being vessel or background). The encoder\nweights were initialized from the foundation model, providing a strong starting point\nfor the supervised learning. During fine-tuning training, we did not apply any data\naugmentations to the input patches. This is a deliberate choice to ensure that the su-\npervised training data (which are relatively limited and precisely annotated) are used\nin their original form. Each training iteration involved feeding a batch of real, 128×128\npatches (each containing at least one ship, as described in Section 3.3.1 through the\nmodel and computing the segmentation loss. We used a binary cross-entropy loss on\nthe output probability mask (comparing each pixel to the ground-truth mask label of\nvessel vs water), which penalizes incorrect classification of pixels. The Adam optimizer\nwas used for fine-tuning as well, with a lower learning rate for the encoder (to avoid\ndrastic changes to the pretrained features) and a higher learning rate for the decoder\n(to allow the new layers to learn quickly). We did monitor performance on a valida-\ntion set throughout training and employed early stopping: if the validation loss did\nnot improve for a certain number of epochs (patience), training was halted to prevent\noverfitting (in practice, fine-tuning stopped after about 60 epochs out of an initially\nset 100, as validation loss plateaued).\n3.3.1 Training Data for Fine-Tuning\nFor the supervised fine-tuning, we compiled a dataset of image patches that contain\nships. Rather than training on all patches (most of which are just empty ocean or land),\nwe selected only patches that had at least one ship pixel in the ground truth\nmask. This focusing was done to ensure the model sees positive examples of ships in\nevery batch and learns to identify the ships’ features, rather than being overwhelmed\nby vast amounts of all-background imagery. (In our dataset, ships are relatively rare\ntargets, so an unbalanced training set with many empty patches could lead to a trivial\nmodel that always predicts “no vessel”.) By restricting training to patches with ships,\nwe force the model to learn the difference between ship pixels and the surrounding\nwater/land context. There is a slight risk that excluding completely-empty patches\ncould increase false alarms (since the model sees fewer examples of pure background),\nbut the unsupervised pretraining stage has already exposed the encoder to numerous\nbackground-only areas, mitigating this concern. We also include patches that have\nvery small ship areas (even 1–2 pixels of a ship) so that the model learns to detect even\ntiny vessels.\n3.3.2 Fine-Tuning Procedure\nWe chose a binary segmentation formulation: each pixel is classified as either vessel\n(1) or water (0). During training, this is treated as a pixel-wise binary classification\nproblem. We applied a sigmoid activation to the output logits and used a binary\ncross-entropy loss (equivalent to per-pixel logistic regression) comparing the output\nmask to the ground truth mask for the patch. This loss penalizes any deviation from\nthe correct label at each pixel. (In preliminary experiments we also considered a Dice",
  "image": "page_040.png"
}