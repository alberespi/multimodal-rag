{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 36,
  "text": "20 CHAPTER 3. MATERIALS & METHODS\n3.1.4 Radiometric Normalization\nTo put multi-scene imagery on a comparable scale and mitigate extreme outliers, we\nperformed per-band normalization using robust percentile stretching. For each spectral\nband, we computed the 2nd and 98th percentile values across all valid pixels (ignoring\nzeros/NoData and NaNs). Pixel values below the 2nd percentile were raised to the 2%\nvalue, and those above the 98th percentile were lowered to the 98% value, effectively\nclipping the tails of the distribution. We then linearly scaled each band so that the\n2nd percentile maps to 0.0 and the 98th percentile to 1.0. Mathematically, for a given\nband we calculate pct2 and pct98, and apply:\nxnorm = x − pct2\npct98 − pct2 (3.1)\nclipping the result to [0, 1]. This yields dimensionless normalized reflectance values.\nBy excluding zero values (which indicate NoData) from the percentile calculation, we\nensure that masked regions do not skew the statistics. After normalization, NoData\npixels remain at 0 (since they were initially 0 and get clipped to 0), whereas valid data\nare in the [0, 1] range. This normalization strategy (sometimes called percentile clip-\nping) preserves relative reflectance differences while reducing the influence of extreme\npixel values (e.g. glare or deep shadows).\n3.1.5 Patch Tiling\nGiven the high resolution and large size of PlanetScope scenes, we adopted a patch-\nbased approach for model training. Each corrected and normalized image was divided\ninto non-overlapping square patches of 128 ×128 pixels (which at 3m resolution corre-\nsponds to roughly 384 ×384m on the ground). We slid a fixed 128px grid over each\nscene without overlap, partitioning it into an array of patches. Only full patches of size\n128×128 (no partial edges) were considered, effectively discarding a thin border if the\nimage dimensions were not exact multiples of 128. We further filtered patches to retain\nonly those that were entirely valid (10% NoData-free). In practice, for each candidate\npatch we checked that it contained no masked pixels – if any pixel was flagged as No-\nData (value 0 after normalization), that patch was excluded from the “full” training\nset. This was implemented by verifying np.all(patch > 0) on the normalized patch\narray. Patches that passed this criterion were saved for use in model training (others\nwere either discarded or kept only for analysis). The rationale was to avoid introduc-\ning any cloudy or missing data during training, so the model learns from clear imagery\nonly in a first training. As a result, our foundation model pretraining used the subset\nof patches with 100% valid data, ensuring that the network isn’t distracted by blank\nor corrupted inputs. This yielded a large collection of image patches (on the order of\ntens of thousands of patches in total) for the self-supervised learning stage. Then, the\nfoundation model was retrained with all kind of data to ensure there is noi bias in the\nfirst training and to make the model more robust in real-case scenarios.\nPatch validity thresholds.\nDuring preprocessing three candidate datasets were produced, keeping only patches\nwith at least 80%, 90%, or 100 % valid pixels, respectively. Although the 80% and\n90% sets increased the sample count by 15–25%, early experiments revealed a slight",
  "image": "page_036.png"
}