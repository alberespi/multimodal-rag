{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 30,
  "text": "14 CHAPTER 2. RELATED WORK\ndatasets, some challenges like Xview [58] and DOTA [59] include “ship” as one\nof multiple object classes in overhead imagery. For example, DOTA (Dataset for\nObject Detection in Aerial Images) has thousands of images with objects rang-\ning from vehicles to buildings and ships, providing a broader test of detection\nalgorithms in mixed contexts [59].\nEach dataset typically comes with a defined train/test split and sometimes a bench-\nmark leaderboard. Using common datasets allows fair comparison of algorithms. It’s\nimportant to note dataset biases: e.g., methods tuned on the calm, largely clear im-\nages of the Airbus dataset might falter on the more challenging MASATI images with\ntough seas and clouds. Therefore, robust evaluation should test an algorithm on mul-\ntiple datasets or on a held-out region it hasn’t seen in training.\n2.3.2 Evaluation Metrics\nTo quantitatively asses detection performance, standard metric from object detection\nand segmentation are used:\n• Precision and Recall: Precision (the fraction of detection that are correct) and\nrecall (the fraction of true vessels that are detected) are fundamental metrics.\nHigh precision means few false alarms, and high recall means few missed targets.\nOften a Precision-Recall curve is plotted, and the balance of the two is considered.\nIn maritime surveillance, precision is crucial to avoid overwhelming operators\nwith false alerts, but recall is equally critical since a missed detection could mean\nan undetected illicit vessel.\n• F1-Score: This is the harmonic mean of precision and recall. It provides a single\nmeasure that balances the two (F1 is high only if both precision and recall are\nhigh). Some papers report F1-score at a particular operating threshold, especially\nif optimizing for a trade-off.\n• Intersection over Union (IoU): In object detection, a detection is considered\na “true positive” if the overlap between the predicted bounding box and the\nground truth box exceeds a threshold (commonly 0.5 or 50%). IoU is the area\nof overlap divided by area of union of the boxes. This metric is also used for\nsegmentation mask overlap. IoU is important because it measures how accurately\nthe location and size of the vessel is predicted, not just whether the object is\nfound. Competitions often use IoU thresholds (e.g., IoU ≥ 0.5) to define correct\ndetections.\n• Average Precision (AP) and mean AP: Average Precision is the area un-\nder the precision-recall curve for a given class. In detection challenges (e.g.,\nCOCO [41] or DOTA [59]), one computes AP for each class and the takes the\nmean (mAP). For ship detection where there is essentially one class (“ship”),\nAP is equivalent to the area under the PR curve for that class. Many works\nreport mAP at IoU=0.5 (Pascal VOC criterion [60]) or the COCO-style mAP\nwhich averages AP across IoU thresholds from 0.5 to 0.95. For instance, one\nstudy reported YOLOv3 achieving 63% mAP and RetinaNet 79% AP on a SAR\nship dataset [7, 20], indicating RetinaNet was more accurate in that test. Higher\nAP/mAP indicates better overall detection performance.",
  "image": "page_030.png"
}