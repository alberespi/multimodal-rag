{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 41,
  "text": "3.4. EVALUATION METRICS & PROTOCOLS 25\nloss, but cross-entropy proved sufficient.) At inference time, the sigmoid output is\nthresholded at 0.5 to produce a binary mask prediction.\nDuring fine-tuning, we train theentire segmentation model end-to-end, choos-\ning in each experiment wether to use pretrained encoder-decoder weights or not. If the\npretrained encoder is very general and good, even freezing it (not updating its weights)\nis an option, but we chose to allow fine-tuning of the encoder with a small learning\nrate to squeeze out additional performance on our specific task. Training proceeds by\nfeeding batches of image patches (with ships) through the model, computing the binary\ncross-entropy loss against the ground truth mask, and backpropagating gradients to\nupdate weights.\nThroughout training, we monitored the model’s performance on a validation set\n(a subset of labeled patches held out from training - see Section 3.4) to decide when\nto stop training (early stopping) and to tune hyperparameters. The fine-tuning pro-\ncess typically converged much faster than training a model from scratch, thanks to the\nstrong initialization from the foundation model. The encoder already “knows” useful\nfeatures (e.g., how water vs land vs man-made objects look), so the training primarily\nneeded to learn to isolate the specific features of ships. This led to significantly im-\nproved data efficiency: our model achieved high accuracy with only a modest number\nof labeled examples, whereas a scratch-trained model struggled. This outcome aligns\nwith reports in the literature that foundation models yield superior downstream task\nperformance with limited data [66].\n3.4 Evaluation Metrics & Protocols\nFor evaluating the segmentation and detection performance, we adopted standard met-\nrics from binary classification and image segmentation. In particular, we report pixel-\nwise accuracy , precision, recall, F1-score, and the Intersection over Union\n(IoU) [67] for the ship class. These metrics are computed by comparing the predicted\nbinary mask (after thresholding the model’s sigmoid output at 0.5) against the ground\ntruth mask on a pixel-by-pixel basis. We define true positives (TP) as pixels correctly\npredicted as ship, true negatives (TN) as pixels correctly predicted as background, false\npositives (FP) as background pixels incorrectly predicted as ship, and false negatives\n(FN) as ship pixels missed by the model. The definitions of these performance metrics\nare summarized in Table 3.1 below.\n3.4.1 Validation and Test Protocol\nWe split our labeled dataset into separate subsets for training and evaluation to ob-\njectively measure model performance. Specifically, we set aside a validation set during\ntraining – a random 20% of the labeled patches (or by holding out certain whole scenes)\n– not used for training updates. This validation set was used to tune hyperparameters\nand perform early stopping. After training, we evaluate the final model on a test set\n(another 20% of the data) that was never seen during training or model development.\nIf data is limited, we perform this as a cross-scene split (ensuring that an entire Plan-\netScope image and its ships are either all in training or all in test, to avoid spatial\nleakage). The test set metrics (accuracy, precision/recall, etc.) represent how well the\nmodel generalizes to new images. In our case, because the total number of labeled",
  "image": "page_041.png"
}