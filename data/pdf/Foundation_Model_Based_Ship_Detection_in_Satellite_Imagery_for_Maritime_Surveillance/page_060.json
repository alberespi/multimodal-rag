{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 60,
  "text": "44 CHAPTER 5. DISCUSSION & EVALUATION\nsetting (IoU thr = 0.20, min size = 2 px) therefore offers a stable recall/precision\ntrade-off without scene-specific tuning.\nDeployment-ready throughput for MARVISION\nOn an NVIDIA A40 the network processes ≈16 six-band patches every 50 ms ,\nenabling analysis of full 2048 × 2048 PlanetScope scenes in near real time. The 61\nM-parameter model (245 MB ONNX) integrates directly with the AIS-fusion node,\nissuing dark-vessel alerts within < 3 minutes of image acquisition.\nThese findings demonstrate thatinstance-level performance, not merely pixel\naccuracy, benefits markedly from self-supervised pre-training and an IoU-\naware loss, yielding an operationally viable detector for medium-resolution maritime\nimagery. Subsequent sections discuss how these results compare with published work,\nexplore remaining limitations, and outline future extensions.\n5.2 Comparative Performance on Ship Detection\nTo put our results in context, we compare them with three reference works on ship\nsegmentation in satellite/aerial imagery: (1) the Airbus Ship Detection Challenge\n(2018) [53], (2) a U-Net model trained on MASATI (a maritime aerial image\ndataset) [54] and applied to HRSC2016 (a high-resolution ship imagery dataset) [68],\nand (3) the recent UOW-Vessel baseline on a large satellite vessel dataset [56].\nTable 5.1 summarizes the key metrics reported, focusing on instance-wise IoU (IoU\naveraged per detected ship instance), pixel-wise IoU (overall semantic segmentation\nIoU) [69], and F-score (or related metrics). Because different works use different evalu-\nation protocols, direct comparisons should be made with caution – we include clarifying\nnotes where metrics do not exactly align.\nTable 5.1: Comparison with recent ship-segmentation works. “Inst. IoU” is the mean\nIoU per vessel instance; “Px. IoU” is the conventional semantic IoU over all pixels.\nWhere a study reports a different primary metric (F2, mAP) the value is shown in the\nright-most column and the IoU cells are left blank (–).\nMethod / Year Dataset Inst. IoU Px. IoU Official metric\nKaggle Airbus baseline UNet\n(2018)[53]\nAirbus (SPOT 6/7) – – F2 @ 0.5–0.95 = 0.82\nMASATI-UNet → HRSC2016\n(2021)[54, 68]\nHRSC (0.41 m RGB) – ∼0.90∗ F1 = 0.945∗\nUOW-Vessel baseline\nYOLOv8-L (2024)[56]\nUOW-Vessel (HR optical) – – mAP0.5:0.95 = 61.7 %\nThis work (BCE+IoU) PlanetScope (3.7 m, 6-band) 0.536 0.356 Inst. F1 = 0.671\n∗ Exact IoU not reported; value estimated from the authors’ confusion matrix.\nTable 5.1 shows that the instance-wise IoU performance metrics used for ship detec-\ntion works. Instance-wise IoU refers to the mean IoU per ship instance [69]; pixel-wise\nIoU is the conventional semantic IoU over all pixels [68]. “F-score” denotes the har-\nmonic mean of precision and recall (F1 or variant).Airbus (2018) [53]: a Kaggle com-\npetition where the metric was an averageF2-score (β = 2) over IoU thresholds 0.5–0.95.\nA basic U-Net achieved F2 ≈0.82 [11], while winning ensembles reached ∼0.93 (unoffi-\ncial reports). MASATI-UNet: Gallego et al.’s MASATI dataset [54] provides labeled",
  "image": "page_060.png"
}