{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 44,
  "text": "28 CHAPTER 3. MATERIALS & METHODS\nNumPy and PyTorch) in all experiments to ensure that results are repeatable (up to\nthe non-determinism of GPU operations).\nComponent Description\nCompute Hardware Internal server with 1 × NVIDIA A40 GPU (48 GB\nVRAM); Dual Intel Xeon CPUs; 32 GB RAM; 2 TB NVMe\nSSD storage.\nSoftware Environment Ubuntu 20.04 LTS; Python 3.10; PyTorch 2.6 (CUDA 11);\nRasterio 1.3; NumPy 1.24; Pandas 1.5; scikit-learn 1.1; etc.\n(Conda environment with pinned versions).\nReproducibility Training code executed in Jupyter/Python scripts, version-\ncontrolled with Git; Fixed random seeds for consistent ex-\nperiment reruns.\nTable 3.2: Hardware and software environment used for training and evaluation of the\nmodel.\n3.5.2 Training Hyperparameters\nWe summarize the key training hyperparameters for both the pretraining and fine-\ntuning stages in Table 3.3. Unless otherwise noted, the same optimizer (Adam) was\nused in both phases. For the fine-tuning stage, when using a pretrained encoder, we\nset a lower learning rate for the encoder weights compared to the decoder, as noted\nbelow.\nHyperparameter Pretraining Fine-tuning\nBatch size 32 16\nInitial LR 1 × 10−3 → 10−4 1×10−4 (encoder)\n5×10−4 (decoder)\nTotal epochs 500 100 (early stop ∼60)\nLoss Masked MSE BCE (pixel)\nOptimizer Adam (0 .9,0.999) Adam (0 .9,0.999)\nAugmentations Rot/Flip —\nTable 3.3: Summary of training hyperparameters and settings for the self-supervised\npretraining stage vs. the supervised fine-tuning stage.\nDuring pretraining, each epoch consisted of roughly 50,000 image patches (covering\ndiverse geographic regions and conditions), and training ran for up to 500 epochs until\nthe reconstruction loss converged (by around epoch 400). The fine-tuning stage, in\ncontrast, used a much smaller labeled dataset of only a few hundred ship-containing\npatches. We ran fine-tuning for at most 100 epochs, but with early stopping the\nbest model was typically obtained around the 50–60 th epoch, beyond which no further\nimprovement on validation data was observed. The batch size in fine-tuning was set\nsmaller (16) than in pretraining (32) due to the larger memory requirements when the\nfull encoder–decoder model is active, and also to allow more frequent validation checks\ngiven the limited data. Using a learning rate of 10 −4 for the encoder (to make only\ngentle updates to the pretrained weights) and a slightly higher rate (e.g., 5 × 10−4)\nfor the new decoder proved effective, as it balanced retaining learned features with\nlearning new task-specific features. All training was accelerated by the GPU, with",
  "image": "page_044.png"
}