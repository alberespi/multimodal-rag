{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 26,
  "text": "10 CHAPTER 2. RELATED WORK\naccuracy. A basic CFAR detector might miss dim or small vessels and is prone to\nfalse alarms from sea clutter [9]. Unsupervised clustering methods, while improv-\ning, typically lag supervised CNNs in detection rates unless carefully engineered\n(EDIC being a notable exception that reported comparable accuracy to deep\nmodels) [26]. Foundation models enhance accuracy especially when labeled\ndata is limited: a fine-tuned foundation model can maintain high recall where\na standard model might fail, because its pre-trained features better distinguish\nships from novel backgrounds [8]. In summary, supervised deep models currently\nset the state-of-the-art accuracy, with foundation model pretraining boosting\ntheir performance further, whereas classical detectors remain inferior in complex\nscenarios.\n• Computational Efficiency: Simpler methods have the edge in speed. CFAR\nand another thresholding techniques run in near real-time even on large images\n(they involve few operations per pixel) and can be deployed on low-power devices.\nDeep learning models are more computationally demanding, often requiring GPU\nacceleration dor real-time inference. Among deep models. one-stage architec-\ntures (YOLO, SSD) are optimized for speed - e.g., YOLOv4 can process ∼65\nframes per second on COCO-sized images [7, 40, 41], and even lightweight ver-\nsions (YOLOv4-tiy) reach hundreds of FPS with some accuracy sacrifice [7, 42].\nTwo-stage detectors are usually slower (Faster R-CNN might run a few FPS or\nless, due to the proposal stage) [7]. The largest foundation models (e.g., Vision\nTransformers with billions of parameters) incur additional computational cost\nduring inference [14], but in practice many foundation models used for detec-\ntion (like ResNet50 with self-supervised weights) have inference cost similar to\na normal CNN. Therefor, using a foundation model does not drastically change\nruntime; the heavy cost is in thepre-training phase which is done offline. Recently\nresearch also explores model compression and efficient backbones (MobileNets,\nknowledge distillation) to deploy vessel detectors on edge devices (drones, nano-\nsatellites) [43, 44, 45]. A point worth noting is that some unsupervised methods\nare extremely fast: the EDIC clustering method, for instance, involves a singular\nvalue decomposition on local windows and k-means clustering, which is much\nlighter than a deep CNN forward pass, thus offering high throughput [26]. In\nsummary, classical detectors are most efficient, one-stage deep detectors strike a\ngood balance of speed and accuracy and foundation-based detectors have similar\nruntime as their architecture type (with potentially larger memory footprint).\n• Generalization to Unseen Data: Generalization refers to how well a method\nperforms on data that differ from its training or design conditions (e.g., a new ge-\nography, sensor or weather situation). Here, foundation models and unsupervised\napproaches offer a clear advantage . Because foundation models are pre-trained\non diverse dataset (potentially including multiple geographies and seasons), they\nimbue the detector with a broad prior knowledge. Fine-tuned foundation models\nhave demonstrated greater robustness to distribution shifts than models trained\nfrom scratch [8]. For example, a model pre-trained via contrastive learning on\nglobal satellite imagery can detect ships in an Arctic scene or a new sensor’s\nimages more reliably than an purely supervised model that has never seen such\ncases. Traditional methods based on physical models (like CFAR) are implicitly\ngeneral in that they don’t learn from data at all - a CFAR detector will apply",
  "image": "page_026.png"
}