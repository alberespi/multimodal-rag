{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 49,
  "text": "4.1. SELF-SUPERVISED PRETRAINING ON UNLABELED IMAGERY 33\nstabilizes gradually as the model continues training. Around epoch 200, both curves\nconverge tightly, with validation loss consistently tracking the training loss without\ndivergence. Final loss values remain in the order of 10 −4, which is one to two orders\nof magnitude lower than those achieved with BCELoss.\nThis difference in scale and convergence behavior underscores the advantage of us-\ning MSELoss for this task. The MSE-trained model not only achieves much lower\nreconstruction error , but it also exhibits smoother and more stable conver-\ngence, with no signs of overfitting. This suggests that MSELoss allows the model to\nmore effectively learn subtle pixel-level structures and inter-band correlations that are\ncritical in multispectral satellite data. By penalizing the squared difference between\npredicted and actual values, MSELoss directly encourages fidelity in pixel intensities,\naligning more naturally with the continuous nature of the data.\nIn summary, these results clearly indicate that MSELoss is the superior choice\nfor the masked image reconstruction task used during foundation model pretraining. Its\nuse leads to faster convergence, lower reconstruction error, and improved generalization.\nThe weights obtained from this training regime serve as a strong initialization for the\ndownstream fine-tuning stage on the ship detection task, discussed in Section 4.2.\n4.1.2 Quantitative Reconstruction Performance and Loss Func-\ntion Impact\nTable 4.1 reports the best validation loss achieved by each loss function during self-\nsupervised pre-training (seen in artifact logs). The best val loss is recorded at the\nepoch where the lowest validation loss was reached.\nTable 4.1: Best validation reconstruction loss for each loss function during self-\nsupervised pretraining.\nLoss Function Best Validation Loss ↓ Epoch of Best Loss\nBCELoss 0.2198 429\nMSELoss 7.9 × 10−5 324\nThe difference between the two objectives is striking:\n• The model trained with MSE reaches a minimum validation loss of 7 .9 × 10−5,\nwhereas the BCE counterpart plateaus at 2.20 × 10−1.\n• In other words, the reconstruction error obtained with MSELoss is ≈ 2.8 × 103\ntimes lower than with BCELoss (0 .2198/7.9 × 10−5 ≈ 2, 800).\nSuch a three-orders-of-magnitude gap confirms that a squared-error objective is far\nbetter aligned with the continuous reflectance values present in multispectral Plan-\netScope imagery. BCE, designed for binary outputs, penalizes only the sign of the\nerror, whereas MSE penalizes its magnitude, encouraging the network to reproduce\nfine-grained intensity variations. The much lower loss obtained with MSELoss there-\nfore indicates a significantly more faithful reconstruction and, by extension, a richer\ninternal representation to transfer to downstream tasks.\no additional image similarity metrics (PSNR, SSIM) were computed because the\norder-of-magnitude difference in the primary loss already provides clear evidence of\nsuperiority. Nevertheless, visual inspection in Section 4.1.3 corroborates that MSELoss",
  "image": "page_049.png"
}