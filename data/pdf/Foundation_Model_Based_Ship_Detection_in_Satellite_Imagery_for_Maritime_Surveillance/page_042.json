{
  "source": "Foundation_Model_Based_Ship_Detection_in_Satellite_Imagery_for_Maritime_Surveillance.pdf",
  "page": 42,
  "text": "26 CHAPTER 3. MATERIALS & METHODS\nMetric Definition\nAccuracy TP + TN\nTP + TN + FP + FN – Fraction of all pixels (ship or back-\nground) correctly classified (overall pixel accuracy).\nPrecision TP\nTP + FP – Fraction of predicted ”ship” pixels that are actual\nships (precision, indicating the model’s specificity).\nRecall TP\nTP + FN – Fraction of true ship pixels correctly predicted as\nship (recall or sensitivity).\nF1-score 2 · Precision · Recall\nPrecision + Recall – Harmonic mean of precision and recall;\nhigh only when both precision and recall are high.\nIntersection\nover Union\n(IoU) [67]\n|P ∩ G|\n|P ∪ G| – Overlap area divided by union area between pre-\ndicted mask P and ground truth mask G (also known as Jac-\ncard index).\nTable 3.1: Summary of evaluation metrics. T P, T N, F P, F Nrefer to the number of\ntrue positive, true negative, false positive, and false negative pixels, respectively, with\n”ship” as the positive class.\nships is not very large, we used roughly 70% of the data for training, 15% for valida-\ntion, and 15% for final testing (approximately; the exact split was adjusted to ensure\neach set had a variety of scenes). We also ensure that the test set includes challenging\ninstances (e.g., very small ships, ships in ports, ships in high waves) to thoroughly\nevaluate performance. All metric calculations described above are performed on the\ntest set predictions vs ground truth.\n3.4.2 Qualitative Evaluation\nBesides numeric metrics, we will examine the model’s output qualitatively. We generate\nvisual comparisons of predicted masks vs ground truth for various test patches.\nFor example, Figure 3.3 shows a PlanetScope image set of patches patch containing at\nleast one ship alongside two binary masks – one from the hand-labeled ground truth\nand one from the model’s prediction. Such visualizations help interpret the results: we\ncan verify if the model generally captures the shape and position of ships correctly, and\nidentify types of errors (e.g., false positives on ship-like clouds or fragmented detection\nof a large ship). We include examples of true positives (ships correctly detected), false\nnegatives (missed ships), and false positives (predicted ships that aren’t real) in the\nthesis to illustrate what the model is getting right and where it struggles. This qual-\nitative assessment is an important complement to the quantitative metrics, providing\nintuition about model behavior in real-world scenarios.\nIn summary, our evaluation strategy is multi-faceted: we quantify overall pixel-level\nperformance, delve into object-level detection success, and visualize results to ensure\nthe model’s outputs make sense. All evaluations adhere to standard practices in the\nfield – for instance, IoU [67] is a widely-used metric in image segmentation and\nour train/val/test splitting follows machine learning validation norms. This rigorous\nevaluation will allow us to judge whether the foundation model approach indeed im-\nproved ship detection performance compared to a baseline.",
  "image": "page_042.png"
}